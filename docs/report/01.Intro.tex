% PACT ABSTRACT Modern optimizing compilers implement robust auto-vectorization
%techniques targeting rich SIMD ISAs. However the typical practice is to
%generate machine-specific assembly code, which exploits the SIMD units of the
%targeted processor.  In this work we take a different approach, by developing
%a source-to-source compilation framework targeting the automatic vectorization
%of specific loop regions. We implement vectorization using an intrinsics-style
%approach, to facilitate portability to a variety of concrete SIMD ISAs. We
%develop machine-independent cost-driven algorithms to pack efficiently
%arbitrary operands and operations into SIMD vectors. Specifically, we support
%vector packing across multiple distinct loop nests to maximize vector
%occupancy in particular when loops have a very small trip count. Experimental
%results are presented for a large set of vectorizable loop shapes, and for
%several key deep learning inference programs, demonstrating the benefits of
%random vector packing for efficient and portable vectorization of highly
%rectangular loops.



%Despite of the extraordinary performance with regard to the optimizations that
%compilers, e.g. ICC, Clang/LLVM, GNU/GCC, may be able to do all over programs,
%they also show limitations when it comes to vectorization. There are cases
%where either they may not vectorize the code at all or even deliver worse
%performance than a sequential version of it. This is a common scenario when
%the region of interest in the code to vectorize is not regular. We may define
%the regularity of a code as the presence of patterns whose accesses to the
%elements (e.g. multi-dimensional arrays) are performed in a sequential and
%contiguous manner, e.g. map operations. On the other hand, reductions are a
%type of operations that, by nature, do not fully exploit the capacity of
%vector operands. Current data placement or packing techniques are meant just
%for loading and storing data from or to memory, without performing any other
%logic than that. Potential improvements regarding the vector occupancy by
%complex and smart packing techniques may lead to major gains in performance.
%Picture the following example: within the same loop we have two different and
%contiguous reductions. This is a common pattern that can be found after the
%fusion of different loops, for instance, in codes that perform convolutions.
%Assuming a vector size of four elements, a smart way of computing this code is
%by



Modern optimizing compilers implement robust auto-vectorization techniques
targeting rich SIMD ISAs. A typical approach is to generate machine-specific
assembly, exploiting SIMD units of a target processor. By default, most of the
techniques employed by auto-vectorizers are conservative, and will only apply
if certain patterns are found in the program~\cite{citeneeded}. Vectorization
also relays in the computation of a cost model. Each compiler has its own
algorithm to compute this value in charge to decide whether the vectorization
is placed or not for a certain region of code. For instance, GCC/GNU and
Clang/LLVM use both Loop Level Vectorizer (LLV) and Superword Level Parallelism
(SLP), the first one just in one pass and the last one in two passes, but both
use a similar approach in order to asses whether the vectorization is
profitable or not: unroll loops using different vector factors, compute de cost
of each vector instruction and compare total cost to the scalar or not
vectorized cost.

% Different ISA
In an orthogonal dimension, the quality of SIMD code is affected by the
knowledge of the architecture where it is being compiled. Nevertheless, some
information regarding the SIMD instructions performance may be missing or
non-disclosed by the manufacturer. This information can be used to determine
whether a vector operation is detrimental or not, i.e. for building a cost
model. There may be architectures using the same ISA and, therefore, where the
vectorization can be done using exactly the same instructions, but because of
their architectural features, performance might be completely different,
leading to the synthesis of particular solutions on each case. Thus, the only
manner of disclosing these features is by reverse-engineering each
architecture. This process is tedious and complex since there are many
different instructions (CISC architectures) whose performance may also vary
depending on the packing of data they use. Nevertheless,
uops.info~\cite{bib:uops} provides performance information of many assembly
instructions in all different variants (order and type of operands, i.e.
register, memory, immediate values, masks, etc.) for various architectures.
This information can be used by compilers to improve their accuracy when
computing its vectorization cost model. Nonetheless, this information is
restricted only to assembly instructions, not those ``high-level'' SIMD
intrinsics such as Intel's (SSE, AVX) or ARM's (NEON, SVE). Main reason is that
not all SIMD macros or idioms have a 1:1 translation onto assembly, but some of
them might generate a set of instructions and/or data movements.


\begin{figure*}
	\centering
	\includegraphics[width=.7\textwidth]{img/MACVETH.pdf}
	\caption{High-level picture of the MACVETH pipeline compiler, based on the
		Clang/LLVM Tool Framework.}
	\label{fig:MACVETHarch}
\end{figure*}

Hence, in this work we take a different approach than ``traditional''
auto-vectorizers, by developing MACVETH, which stands for Multi-dimensional
Array C-compiler for VEctorizating Tensors for HPC applications. This is a
source-to-source compilation framework targeting the automatic vectorization of
specific loop and non-loop regions. We implement vectorization using an
SIMD-intrinsics Â´style approach, to facilitate portability to a variety of
concrete SIMD ISAs. We develop machine-independent cost-driven algorithms to
pack efficiently arbitrary operands and operations into SIMD vectors.
Specifically, we support vector packing across multiple distinct loop nests to
maximize vector occupancy in particular when loops have a very small trip
count. Experimental results are presented for a large set of vectorizable loop
shapes, and for several key deep learning inference programs, demonstrating the
benefits of random vector packing for efficient and portable vectorization of
highly rectangular loops.
